providers:
  ollama:
    enabled: false
    model: llama3
    base_url: http://localhost:11434
    temperature: 0.1
    max_tokens: 4096
  openai:
    enabled: false
    api_key: "${OPENAI_API_KEY}" # Use environment variable
    model: gpt-4-turbo
    temperature: 0.1
    max_tokens: 4096
  anthropic:
    enabled: false
    api_key: "${ANTHROPIC_API_KEY}"
    model: claude-3-sonnet-20240229
    temperature: 0.1
    max_tokens: 4096
  deepseek:
    enabled: true
    api_key: "${DEEPSEEK_API_KEY}"
    model: deepseek-chat
    temperature: 0.1
    max_tokens: 4096
provider_priority:
  - deepseek
  - ollama
  - openai
  - anthropic

retry_settings:
  max_retries: 3
  backoff_factor: 1.0
  status_codes: [429, 502, 503, 504]

rate_limiting:
  requests_per_minute: 60
  tokens_per_minute: 100000

error_handling:
  log_file: logs/errors.log
  max_errors: 1000
  log_level: INFO
  error_tolerance: 0.1
  recovery_enabled: true
